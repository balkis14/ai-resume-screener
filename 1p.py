# -*- coding: utf-8 -*-
"""1P

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19EAxdKgn2-KhF7iBH25OD47coi7cCx6c
"""

import os
import re
import pandas as pd #Pandas is used to work with data easily (tables like Excel)
import numpy as np
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity #It measures how similar two texts are.// Output: number between 0 and 1
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS #A ready-made list of useless words like: the, is, and, in, to, of, for
from collections import Counter

tqdm.pandas()

"""sklearn ::::::It helps you build, train, test, and evaluate ML models easily

tqdm

Why it’s useful (especially for AI projects)

Long loops (cleaning many CVs)

Training steps

Applying preprocessing on big datasets

Instead of waiting blindly… you see progress
"""

RESUME_PATH = "Resume.csv"   # change if different
JOB_PATH = "training_data.csv"         # change if different

resumes_df = pd.read_csv(RESUME_PATH) # to dataframe
jobs_df = pd.read_csv(JOB_PATH)

print("Resumes shape:", resumes_df.shape)
print("Jobs shape:", jobs_df.shape)

"""Raw CSV file (text) → Parsing → DataFrame (structured data)

NLTK = Natural Language Toolkit
It's a Python library for working with human language data (text).
import nltk

# Common NLTK operations:

# 1. **Tokenization** (split text into words)
from nltk.tokenize import word_tokenize
text = "I am a software engineer"
tokens = word_tokenize(text)  # ['I', 'am', 'a', 'software', 'engineer']

# 2. Remove **stopwords** (common words like 'a', 'the', 'is')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
filtered = [w for w in tokens if w.lower() not in stop_words]

# 3. **Stemming** (reduce words to root form) "running", "runs", "ran"
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmer.stem('engineering')  # 'engin'

# 4. Part-of-speech tagging :Identify what type of word each is
nltk.pos_tag(tokens)  # [('I', 'PRP'), ('am', 'VBP'), ...]

***the lookup is faster in sets than in lists***

SETS
# 1. No duplicates
s = {1, 2, 2, 3}  # {1, 2, 3}

# 2. No order (can't use indexing)
s[0]  # ❌ Error! Sets don't have positions

# 3. Fast membership testing
'apple' in my_set  # Very fast!
"""

import nltk
#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

STOPWORDS = set(list(ENGLISH_STOP_WORDS)) - {"not", "no", "nor"}

def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'\s+', ' ', text).strip()

    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r"(\'re)", " are", text)
    text = re.sub(r"(\'s)", " is", text)
    text = re.sub(r"(\'ve)", " have", text)
    text = re.sub(r"(n\'t)", " not", text)
    text = re.sub(r"(\'ll)", " will", text)
    text = re.sub(r"(\'d)", " would", text)
    text = re.sub(r"(\'m)", " am", text)
    text = text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text)
    tokens = [tok for tok in text.split() if len(tok) > 2 and tok not in STOPWORDS] #the result is a list
    return " ".join(tokens) #" ".join() - Combine list into string with spaces

"""re : regular expresion  It's a built-in library for working with patterns in text



re.sub(pattern, replacement, text)
#      ↑        ↑            ↑
#      find /    replace with / in this text
"""

possible_resume_cols = ['resume', 'resume_text', 'text', 'content', 'cv']
possible_job_cols = ['job_description', 'description', 'jd', 'job', 'text']

def find_text_column(df, candidates):
    for c in candidates:
        if c in df.columns:
            return c
    for c in df.columns:
        if df[c].dtype == object:
            return c
    raise ValueError("No suitable text column found. Update candidates or check dataframe columns.")

resume_col = find_text_column(resumes_df, possible_resume_cols)
job_col = find_text_column(jobs_df, possible_job_cols)

print("Using resume column:", resume_col)
print("Using job column:", job_col)

resumes_df['clean_text'] = resumes_df[resume_col].progress_apply(preprocess_text)
jobs_df['clean_text'] = jobs_df[job_col].progress_apply(preprocess_text)

resumes_df[[resume_col, 'clean_text']].head()

"""# WITHOUT parentheses = pass the function itself
resumes_df[resume_col].progress_apply(preprocess_text)
# progress_apply will CALL it later for each row

# WITH parentheses = call the function immediately
resumes_df[resume_col].progress_apply(preprocess_text())  # ❌ WRONG!
# This tries to call preprocess_text() NOW (no argument given → error)

# Pseudo-code of what progress_apply does:
def progress_apply(function):
    results = []
    for each_row in resumes_df[resume_col]:  # Loop through column
        result = function(each_row)           # Pass row to YOUR function
        results.append(result)
    return results

def preprocess_text(text):

resumes_df['clean_text']
 = resumes_df[resume_col].progress_appl(preprocess_text)     

progress_appl : shows a progress bar  with tqdm library
"""

#This code loads a pre-trained model for converting text into numerical vectors ****(embeddings)*****
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"  #Model name: all-MiniLM-L6-v2 Source: From the sentence-transformers library Purpose: Converts sentences/paragraphs into embeddings (vectors of numbers)


model = SentenceTransformer(MODEL_NAME)
BATCH_SIZE = 64

def encode_texts(texts, batch_size=BATCH_SIZE):
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
        batch = texts[i:i+batch_size]
        emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)
        embeddings.append(emb)
    return np.vstack(embeddings)

resume_texts = resumes_df['clean_text'].fillna("").tolist()
job_texts = jobs_df['clean_text'].fillna("").tolist()

resume_embeddings = encode_texts(resume_texts)
job_embeddings = encode_texts(job_texts)

print("Resume embeddings shape:", resume_embeddings.shape)
print("Job embeddings shape:", job_embeddings.shape)+